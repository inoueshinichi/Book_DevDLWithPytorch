{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch用のDataLoaderを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloader_dict(batch_size, train_dataset=None, val_dataset=None, train_shuffle=True):\n",
    "    \n",
    "    # PytorchのDataLoaderに使用するcollate_fnのオーバーライド\n",
    "    def object_detecte_collate_fn(batch):\n",
    "        \"\"\"\n",
    "        Datasetから取り出すアノテーションデータのサイズが画像ごとに異なります。\n",
    "        画像内の物体数が2個であれば(2, 5)というサイズですが、3個であれば（3, 5）など変化します。\n",
    "        この変化に対応したDataLoaderを作成するために、\n",
    "        カスタイマイズした、collate_fnを作成します。\n",
    "        collate_fnは、PyTorchでリストからmini-batchを作成する関数です。\n",
    "        ミニバッチ分の画像が並んでいるリスト変数batchに、\n",
    "        ミニバッチ番号を指定する次元を先頭に1つ追加して、リストの形を変形します。\n",
    "        \"\"\"\n",
    "    \n",
    "        # batch -> img and true_boxes_labels\n",
    "        targets = []\n",
    "        imgs = []\n",
    "        for sample in batch:\n",
    "            imgs.append(sample[0])    # sample[0]は画像\n",
    "            targets.append(sample[1]) # sample[1]はtrue_boxes_labels\n",
    "        \n",
    "        # imgsはミニバッチサイズのリストになっています\n",
    "        # リストの要素はtorch.Size([3, 300, 300])です。\n",
    "        # このリストをtorch.Size([batch_num, 3, 300, 300])のテンソルに変換します\n",
    "        imgs = torch.stack(imgs, dim=0)\n",
    "    \n",
    "        # targetsはアノテーションデータの正解であるgtのリストです。\n",
    "        # リストのサイズはミニバッチサイズです。\n",
    "        # リストtargetsの要素は [n, 5] となっています。\n",
    "        # nは画像ごとに異なり、画像内にある物体の数となります。\n",
    "        # 5は [xmin, ymin, xmax, ymax, class_index] です\n",
    "        return imgs, targets\n",
    "    \n",
    "    \n",
    "    # training\n",
    "    train_dataloader = data.DataLoader(train_dataset, \n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=shuffle,\n",
    "                                       collate_fn=object_detecte_collate_fn)\n",
    "    \n",
    "    # valdation\n",
    "    val_dataloader = data.DataLoader(val_dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=False, # validataionデータはシャッフルしない\n",
    "                                     collate_fn=object_detecte_collate_fn)\n",
    "    \n",
    "    dataloader_dict = { \"train\" : train_dataloader, \"val\" : val_dataloader }\n",
    "    \n",
    "    return dataloader_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'VOCDataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d2c2043ee6f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# 4) Pytorch用のデータセットを作成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mSSD_make_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVOCDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m train_dataset = SSD_make_dataset.VOCDataset(train_img_list, \n\u001b[1;32m     24\u001b[0m                            \u001b[0mtrain_anno_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'VOCDataset'"
     ]
    }
   ],
   "source": [
    "# 1) 学習データ\n",
    "from SSD_make_dataset import make_datapath_list\n",
    "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(rootpath)\n",
    "\n",
    "# 2) アノテーション変換\n",
    "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
    "                   'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "                   'cow', 'diningtable', 'dog', 'horse',\n",
    "                   'motorbike', 'person', 'pottedplant',\n",
    "                   'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "from SSD_convert_xml_annotation_data import Anno_xml2list\n",
    "transform_anno = Anno_xml2list(voc_classes)\n",
    "\n",
    "# 3) 前処理(data_augumentationを含む)\n",
    "color_mean = (104, 117, 123) # VOCデータセットの(BGR)平均\n",
    "input_size = 300\n",
    "from SSD_data_argmentation import DataTransform\n",
    "transform = DataTransform(input_size, color_mean)\n",
    "\n",
    "# 4) Pytorch用のデータセットを作成\n",
    "from SSD_make_dataset import VOCDataset\n",
    "train_dataset = VOCDataset(train_img_list, \n",
    "                           train_anno_list, \n",
    "                           phase='train',\n",
    "                           transform=transform,\n",
    "                           transform_anno=transform_anno)\n",
    "val_dataset = VOCDataset(val_img_list,\n",
    "                         val_anno_list,\n",
    "                         phase='val',\n",
    "                         transform=transform,\n",
    "                         transform_anno=transform_anno)\n",
    "\n",
    "# 5) データローダーの作成\n",
    "batch_size = 5\n",
    "dataloader_dict = make_dataloader_dict(batch_size=batch_size,\n",
    "                                       train_dataset=train_dataset,\n",
    "                                       val_dataset=val_dataset,\n",
    "                                       train_shuffle=True)\n",
    "\n",
    "# 6) イテレータの設定\n",
    "batch_iterator = iter(dataloader_dict[\"val\"]) # イテレータに変換\n",
    "imgs, targets = next(batch_iterator)          # 1番目の要素を取り出す\n",
    "print(imgs.size()) # torch.Size([4, 3, 300, 300])\n",
    "print(len(targets))\n",
    "\n",
    "\n",
    "# 7) ミニバッチサイズのリスト\n",
    "for i in range(batch_size):\n",
    "    print(\"targets[{}]: {}\".format(i, targets[i]))\n",
    "    \n",
    "# 8) データ数\n",
    "print(\"train_dataset_len: {}\".format(train_dataset.__len__()))\n",
    "print(\"val_dataset_len: {}\".format(val_dataset.__len__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyファイルでは同じプログラムで'VOCDataset'をインポートできたので、ここで終了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
